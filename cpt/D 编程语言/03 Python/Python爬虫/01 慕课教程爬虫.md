爬虫库

requests

parsel


beautiful soup
lxml
html5lib
scrapy
selenium



# 爬虫简介

一段自动抓取互联网信息的程序。

互联网数据，为我所用，比如新闻聚合阅读器，爆笑故事app，美女图片，图书价格对比网，技术文章大全



# 简单爬虫架构

爬虫调度端（启动监视和结束），URL管理器（对已爬取和待爬取），下载器，网页解析器，摘取有价值的数据，获取其他URL

**调度器**

URL管理器，待抓取的URL集合和已抓取的URL集合。防止重复抓取，防止循环抓取

添加到集合，判断是否在集合中，判断是否还有待爬取的，获取待爬取，从待爬取移动到已爬取

使用set()存取，使用MySQL存取，使用Redis存取

**下载器**

将网页HTML下载到本地，本地文件或字符串

urllib2基础模块，requests第三方

Urllib2.urlopen(url)

response.getcode()获取成功

response.read()读取内容

urllib2的下载方法

url，data，header添加到入urllib2.request类

urllib2.urlopen(request)发送请求

登录使用HTTPCookieProcessor

代理访问ProxyHandler

加密访问HTTPSHandler

HTTPRedirectHandler

```python
看不懂，先记下来
# 添加特殊情景的处理器
import urllib2,cookielib
# 创建cookie容器
cj = cookielib.CookieJar()
# 创建1个opener
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
# 给urllib2安装opener
urllib2.install_opener(opener)
# 使用带有cookie的urllib2访问网页
response = urllib2.urlopen("http://www.baidu.com/")
```



**解析器**

从网页中提取有价值数据的工具

新的url列表，价值数据

正则表达式，html.parser，Beautiful Soup，lxml

结构化解析，将整个文档解析成DOM树

**应用**



# URL管理器







# 网页下载器urllib2

[Library中文标准库](https://docs.python.org/zh-cn/3/library/index.html)

[urllib.request标准库](https://docs.python.org/zh-cn/3/library/urllib.request.html)

# 网页解析器BeautifulSoup

python第三方插件







# 参考资料

[慕课网python爬虫](https://www.imooc.com/video/10682)

7-2是重点

2020-02-28







# 其他

python下载文件

[python下载文件的三种方法](https://www.cnblogs.com/x00479/p/11274733.html)

