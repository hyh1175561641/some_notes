






https://ollama.com/

https://github.com/ollama/ollama/tree/main/docs










# 资讯


https://blog.csdn.net/2402_84466582/article/details/140765633


大模型框架汇总：大模型框架Ollama、大模型框架vLLM、大模型框架LightLLM、大模型框架llama.cpp、大模型框架LocalAI、大模型框架veGiantModel


常见的大模型框架：大模型框架Ollama、大模型框架vLLM、大模型框架LightLLM、大模型框架llama.cpp、大模型框架LocalAI、大模型框架veGiantModel：

大模型框架是指用于训练、推理和部署大型语言模型（LLMs）的软件工具和库。这些框架通常提供了高效的计算资源管理、分布式训练、模型优化和推理加速等功能，以便更好地利用硬件资源（如GPU和TPU）来处理庞大的数据集和复杂的模型结构。以下是对大模型框架的详细阐述：

一、大模型框架的主要特点

高效性：通过优化计算和内存管理，大模型框架能够显著提高训练和推理的速度。

可扩展性：支持分布式训练，可以在多个GPU或TPU上运行，适用于大规模数据集和复杂任务。

灵活性：提供丰富的API和工具，使得研究人员和工程师可以方便地进行定制化开发。

易用性：通常具有良好的文档和社区支持，降低了使用门槛。

    常见的大模型框架

    Ollama大模型框架

Ollama大模型框架是一个专注于简化大型语言模型（LLM）在本地部署和运行的开源框架。以下是Ollama的详细介绍：

1）、主要功能与特点

简化部署：

Ollama使用Docker容器技术来简化LLM的部署过程，使得用户无需深入了解底层复杂性即可快速启动和运行模型。

用户只需执行简单的命令即可在本地计算机上部署和管理LLM，降低了技术门槛。

捆绑模型组件：

Ollama将模型权重、配置和数据捆绑到一个包中，称为Modelfile，这有助于优化设置和配置细节，包括GPU使用情况。

这种捆绑方式使得用户能够更轻松地管理和切换不同的模型。

支持多种模型：

Ollama支持多种大型语言模型，如Llama 2、Code Llama、Mistral、Gemma等，并允许用户根据特定需求定制和创建自己的模型。

这为用户提供了丰富的选择，并促进了LLM技术的多样性和创新。

跨平台支持：

Ollama支持macOS和Linux平台，Windows平台的预览版也已发布。

这使得不同操作系统的用户都能够利用Ollama来部署和运行LLM。

命令行操作：

安装完成后，用户可以通过简单的命令行操作启动和运行大型语言模型。

例如，要运行Gemma 2B模型，只需执行命令ollama run gemma:2b。

资源要求：

为了顺畅运行大模型，Ollama需要一定的内存或显存资源。

例如，至少需要8GB的内存/显存来运行7B模型，至少需要16GB来运行13B模型，至少需要32GB来运行34B的模型。

2）、使用场景与优势

使用场景：

Ollama可以应用于多种场景，如聊天机器人、文本生成、问答系统等。

它为研究人员、开发人员和爱好者提供了一个强大的工具来探索和使用LLM技术。

优势：

易用性：Ollama提供了简洁的API和类似ChatGPT的聊天界面，使得用户无需开发即可直接与模型进行交互。

轻量级：Ollama的代码简洁明了，运行时占用资源少，适合在本地计算机上运行。

可扩展性：Ollama支持多种模型架构，并可以扩展以支持新的模型。它还支持热加载模型文件，无需重新启动即可切换不同的模型。

预构建模型库：Ollama提供了一个预构建模型库，涵盖了各种自然语言处理任务，如文本生成、翻译、问答等。

3）、未来展望

随着LLM技术的不断发展，Ollama也在不断完善和扩展其功能。未来，Ollama可能会支持更多流行的模型架构和预训练模型，提供更多的自定义选项和高级功能。同时，它还将继续优化性能，提高运行速度和降低资源占用，以满足不同用户的需求。

综上所述，Ollama是一个功能强大、易于使用且可扩展的大模型框架，它为在本地部署和运行LLM提供了便捷的解决方案。

2、大模型框架vLLM（Vectorized Large Language Model Serving System）

大模型框架vLLM是一个高效的大模型推理与服务引擎，专为大型语言模型（LLM）打造，旨在提升推理速度、降低显存占用，并更好地满足实际应用需求。以下是对vLLM的详细介绍：

1）、概述

vLLM是一个基于Python的LLM推理和服务框架，由伯克利大学LMSYS组织开源。它通过创新的PagedAttention技术、连续批处理、CUDA核心优化以及分布式推理支持，显著提高了LLM的推理性能。vLLM不仅简单易用，而且性能高效，广泛应用于各种NLP任务中。

2）、核心优势

PagedAttention技术：

通过内存管理技术，PagedAttention能够将注意力机制中的键（keys）和值（values）存储在不连续的显存空间中，从而减少显存碎片，提高显存利用率。

吞吐量最多可以达到Hugging Face实现的24倍，文本生成推理（TGI）高出3.5倍，且无需修改模型结构。

连续批处理：

vLLM能够连续批处理接入的请求，充分利用GPU资源，提高吞吐量。

CUDA核心优化：

针对CUDA核心进行了优化，确保速度与效率。

分布式推理支持：

支持分布式推理，能够在多台GPU上并行运行模型，进一步提高推理速度。

3)、功能特点

支持多种模型格式：

vLLM框架支持多种模型格式，包括PyTorch、TensorFlow等，方便用户根据自己的需求选择合适的模型。

高性能推理引擎：

提供高性能的推理引擎，支持在线推理和批量推理，能够快速响应大量并发请求。

灵活的接口：

提供丰富的接口和输出格式，方便用户将推理结果集成到实际应用中。

易于部署：

部署过程简单快捷，用户只需按照文档说明进行操作即可快速部署vLLM框架。

4)、应用场景

vLLM框架广泛应用于各种NLP任务中，包括但不限于文本分类、情感分析、问答系统、文本生成等。通过vLLM的推理加速，用户可以更快地处理大量自然语言数据，提高业务效率。

5)、部署与配置

在部署vLLM之前，用户需要准备相应的硬件和软件环境。硬件方面，建议使用高性能的服务器，具备足够的CPU、内存和GPU资源。软件方面，需要安装Python 3.x环境，并配置好vLLM框架所需的依赖库。

部署步骤通常包括：

安装虚拟环境（如conda或virtualenv）。

安装必要的依赖库（如torch、transformers等）。

下载vLLM源码并解压到本地目录。

配置模型参数，包括模型路径、输入输出数据类型等。

启动vLLM服务，并指定监听端口等参数。

6)、总结

vLLM大模型框架

vLLM大模型框架作为一款高效的大模型推理与服务引擎，通过创新的PagedAttention技术、连续批处理、CUDA核心优化以及分布式推理支持，显著提高了LLM的推理性能。其简单易用、性能高效的特点使得vLLM在NLP领域具有广泛的应用前景。随着技术的不断发展，vLLM框架将继续升级和完善，为更多的模型和应用提供更好的支持。

3、大模型框架LightLLM：

大模型框架LightLLM是一个基于Python的轻量级、高性能的LLM（大型语言模型）推理和服务器框架。以下是对LightLLM的详细介绍：

1）、概述

LightLLM旨在通过其独特的设计、易扩展性和高效率，为LLM的推理和应用提供强有力的支持。它借鉴并整合了FasterTransformer、TGI、vLLM和FlashAttention等优秀开源实现，为用户提供一个全新的LLM服务模式。

2）、核心特点

三进程异步协作：

LightLLM采用三进程架构，将词法化（tokenize）、模型推断和词法还原（detokenize）三大步骤解耦，通过异步协作的方式运行。这种设计极大地提高了GPU的利用率，减少了数据传输带来的延迟。

Nopad无填充操作：

LightLLM支持Nopad无填充操作，能够更有效地处理长度差异较大的请求，避免了无效填充，从而提高了资源利用率。

动态批处理：

系统能够动态调整请求批次的大小，以适应不同长度的输入请求，进一步提升性能。

FlashAttention集成：

通过集成FlashAttention技术，LightLLM在保持高速运行的同时，还能有效降低GPU内存使用，优化推理速度。

Token Attention：

LightLLM引入了一种以Token为粒度进行kv cache显存管理的特性，通过高性能的算子和高效的显存申请释放方式，有效管理模型推理过程中的显存占用，减少显存碎片化问题。

Tensor Parallelism：

支持多GPU并行计算，加速推理过程，提高整体处理效率。

Int8KV Cache：

扩大令牌容量，提高系统效率，使得LightLLM能够处理更大规模的数据。

3）、应用场景

LightLLM适用于各种需要LLM的场合，包括但不限于：

聊天机器人

文本生成

问答系统

代码补全

自然语言理解

无论是在云端大规模服务部署，还是在边缘设备上进行实时推理，LightLLM的轻量级设计和高效率都能发挥巨大价值。

4）、性能优势

LightLLM在大部分场景下能够获得比其他框架更高的吞吐量，甚至可以达到4倍左右的性能提升。这得益于其高效的架构设计和多种优化特性。

五、易用性与扩展性

LightLLM是一个纯Python框架，代码结构清晰，易于理解和二次开发。它支持扩展对不同模型的支持，并提供了详细的文档和示例代码，包括基本环境要求、Docker容器使用、源码安装等。用户只需几行命令，就可以快速启动一个LightLLM服务器，并开始尝试LLM的强大功能。

六、总结

LightLLM作为一个轻量级、高性能的LLM推理和服务器框架，以其独特的设计、高效的性能和易用性，在LLM的推理和应用领域展现出了巨大的潜力。无论是开发者还是研究者，都可以通过LightLLM来构建高效的服务或寻求强大的推理工具，从而推动人工智能技术的发展。

    大模型框架llama.cpp

大模型框架llama.cpp是一个由Georgi Gerganov开发的开源工具，旨在优化语言模型在多种硬件上的推理性能。以下是对llama.cpp的详细介绍：

1）、概述

开发目的：llama.cpp主要解决大模型推理过程中的性能问题，通过优化和量化技术，提高模型在CPU、GPU等硬件上的推理速度和效率。

应用场景：适用于在各种硬件平台上部署和运行大型语言模型（LLMs），支持AI应用的开发和研究。

2）、核心功能

高性能推理引擎：

使用C语言编写的机器学习张量库ggml，高效处理大规模的张量运算，加速模型推理。

支持CPU、CUDA和OpenCL，能够在多种硬件上运行，包括桌面计算机、服务器和某些移动设备。

模型量化工具：

允许用户将原本的32位浮点数模型参数量化为16位浮点数，甚至是更低精度的8位或4位整数。

通过牺牲一定的模型精度来换取推理速度的提升，特别适用于资源受限的设备。

服务化组件：

提供服务化组件，可以直接对外提供模型的API，便于用户构建自己的应用。

支持以server或命令行方式运行模型，方便用户根据自己的需求选择合适的运行方式。

多格式支持：

支持多种模型格式，包括PyTorch的.pth、huggingface的.safetensors以及llama.cpp采用的ggmlv3等。

使得用户能够轻松地将不同来源的模型转换为llama.cpp支持的格式并进行推理。

3）、使用方法

编译完成后，会生成一系列可执行文件，包括用于推理的main、用于量化的quantize以及提供API服务的server等。

模型准备与转换：

用户需要准备llama.cpp支持的模型文件，并将其放置在指定的目录下。

使用项目提供的转换工具（如convert.py）将模型转换为llama.cpp支持的gguf格式。

模型量化：

使用quantize工具对模型进行量化，选择合适的量化精度以满足性能和精度的需求。

量化后的模型文件将被保存到指定的位置，供后续推理使用。

模型推理：

使用main或llama-cli（取决于版本）进行模型推理，输入相应的文本并获取推理结果。

也可以使用llama-server启动一个HTTP服务器，通过API方式提供模型服务。

4）、社区与生态

社区支持：llama.cpp在GitHub上拥有大量的stars和关注者，表明其在开发者社区中的高关注度和活跃度。

生态发展：随着社区的发展，llama.cpp生态圈中出现了多个项目，如chatglm.cpp、koboldcpp等，支持不同的模型和功能扩展。

5)、总结

llama.cpp是一个专为性能优化和广泛兼容性设计的工具，它为用户提供了一个高性能、灵活且易于使用的平台，用于在各种硬件平台上部署和运行大型语言模型。通过其高性能的推理引擎、模型量化工具、服务化组件以及多格式支持等功能，llama.cpp能够帮助研究人员和开发者更加高效地开发和部署AI应用。

    大模型框架LocalAI

LocalAI是一个开源项目，旨在帮助开发者在本地环境中快速搭建和运行人工智能应用，无需云端依赖。以下是对LocalAI的详细介绍：

一、项目概述

目标：LocalAI的目标是让开发者能够在本地环境中轻松实现智能化应用，无需担心云端服务的成本和数据安全问题。

发起者：该项目由Mudler发起，并由社区持续维护和更新。

特点：提供了一套简单易用的接口和工具，使得非AI专业人员也能轻松地将AI功能集成到自己的项目中。

二、核心功能

模型管理：

LocalAI提供了多种预训练模型，如图像识别、文本分类等，并进行了封装，用户只需几行代码即可调用。

支持自定义或新模型的导入，允许用户利用TensorFlow、PyTorch等深度学习库构建的模型。

离线运行：

所有计算都在本地进行，数据无需上传至云服务器，保护了用户的隐私和数据安全。

API接口：

设计了一套简洁明了的API接口，方便开发者快速集成到现有系统。

兼容OpenAI API规范，提供了一个无缝的、无需GPU的OpenAI替代品。

跨平台支持：

支持多种操作系统，包括Windows、macOS和各种Linux发行版。

性能优化：

使用C++绑定来实现更快的推理和更好的性能。

可以在消费级硬件上运行，无需GPU加速，但支持GPU加速以提高计算速度和能效。

三、应用场景

由于其灵活性和隐私保护特性，LocalAI可广泛应用于以下场景：

智能家居：在物联网设备上实现本地化的语音识别和智能控制。

移动应用：为应用添加实时的图像分析或自然语言处理能力，提高用户体验。

企业内部工具：用于数据分析和报告生成，提升工作效率。

隐私敏感项目：如医疗健康领域，确保患者数据的安全性。

四、使用方式

安装与部署：

LocalAI通常以容器镜像的形式出现，可以使用Docker等工具进行部署。

用户需要下载相应的镜像文件，并配置好本地环境，如设置模型路径等。

模型运行：

通过API接口调用预训练模型或自定义模型进行推理。

支持多种模型格式，如ggml等。

测试与验证：

用户可以使用API调试工具（如Postman）或命令行工具（如curl）来测试API接口是否正常工作。

通过发送请求并接收响应来验证模型的推理结果。

五、社区与生态

社区支持：LocalAI是一个社区驱动的项目，拥有活跃的开发者社区支持。

生态发展：随着社区的发展，LocalAI的功能和性能将不断优化和扩展。

总之，LocalAI是一个功能强大且易于使用的本地AI框架，为开发者提供了在本地环境中快速实现智能化应用的便利。通过其丰富的预训练模型、简洁的API接口以及跨平台支持等特点，LocalAI在智能家居、移动应用、企业内部工具等多个领域具有广泛的应用前景。

6、大模型框架veGiantModel详细介绍

大模型框架veGiantModel是字节跳动AML团队内部开发的一款高性能大模型训练框架，它主要针对当前大模型训练中的显存压力、计算压力和通信压力等挑战进行了优化。以下是对veGiantModel的详细介绍：

一、背景与需求

随着NLP（自然语言处理）领域的快速发展，Bert、GPT、GPT-3等超大模型在各类NLP测试中取得了显著成绩，人们发现参数量越大的模型在算法方面表现越好。因此，模型体积呈现爆炸式增长，这对现有的训练系统带来了巨大挑战。veGiantModel正是为了应对这些挑战而开发的。

二、技术特点

高性能与扩展性：

veGiantModel基于PyTorch框架，以Megatron和DeepSpeed为基础构建，同时支持数据并行、算子切分、流水线并行三种分布式并行策略。

支持自动化和定制化的并行策略，可以根据具体需求进行灵活调整。

基于字节跳动自研的高性能异步通讯库ByteCCL（BytePS的升级版），训练任务吞吐相比其他开源框架有1.2x-3.5x的提升。

友好的流水线支持：

veGiantModel提供了更友好、灵活的流水线支持，降低了模型开发迭代所需要的人力。

可以在GPU上高效地支持数十亿至上千亿参数量的大模型训练。

低带宽要求：

对网络带宽要求低，在私有化部署时无RDMA强依赖。

在不同带宽环境下，veGiantModel的吞吐性能受带宽变化影响相对较小。

三、性能测试与对比

为了展示veGiantModel的性能，团队在自建机房的物理机上进行了测试，分别使用了Tesla V100和Ampere A100两种型号的GPU。测试结果显示，无论是在V100还是A100上，veGiantModel的性能均优于Megatron和DeepSpeed，最高可达6.9倍的提升。特别是在高带宽和低带宽环境下，veGiantModel均表现出色，对带宽变化的敏感度较低。

四、应用场景

veGiantModel主要应用于自然语言处理领域的大模型训练，能够显著提升训练效率和性能。随着大模型和超大模型在AI领域的广泛应用，veGiantModel有望成为推动AI技术发展的重要工具。

五、开源与支持

目前，veGiantModel已在GitHub上开源，同时，字节跳动旗下的企业级技术服务平台火山引擎已在其机器学习平台上原生支持了veGiantModel，该平台正在公测中。

综上所述，veGiantModel是一款针对大模型训练场景进行优化的高性能训练框架，具有高性能、扩展性强、友好的流水线支持以及低带宽要求等特点。它的出现有望推动AI技术在自然语言处理等领域的进一步发展。
算力资源比较多
关注

    29
    29
    0
    分享

基础篇| 全网最全详解12个大模型推理框架
youbingchen的博客
3236
开始介绍之前, 我们先了解一下什么是框架?xx框架-IT人经常听到的名词。但是又有多少人知道框架的意思?框架（framework）是一个框子:指其约束性，也是一个架子——指其支撑性。是一个基本概念上的结构，用于去解决或者处理复杂的问题。在IT软件领域，软件框架（software framework）的标准定义：通常指的是为了实现某个业界标准或完成特定基本任务的软件组件规范，也指为了实现某个软件组件规范时，提供规范所要求之基础功能的软件产品。
六、大模型开发框架LangChain
挑大梁的专栏
1124
将大语言模型作为一个推理引擎。给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。
基础篇| 全网最全详解12个大模型推理框架
12-19
同时仅仅依靠推理引擎功能,离一个完整大模型应用开发平台还有一段距离, 大模型应用开发平台的工具除了支持基本的模型推理,还有标准化的api,以及配套管理工具,可以方便去开发和管理AI应用。 推理框架作为大模型应用开发平台的引擎, 其重要性不在赘述,毫不夸张的说,为自己应用场景选择一款适合的推理框架,是LLM应用场景开发...
2024年掌握这些大模型应用开发框架,不再失业_大模型开发框架
12-19
• 基于大模型和企业数据AI应用开发,实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能,学会Fine-tuning垂直训练大模型(数据准备、数据蒸馏、大模型部署)一站式掌握; • 能够完成时下热门大模型垂直领域模型训练能力,提高程序员的编码能力:大模型应用开发需要掌握机器学习算法、深度学习框架等技术,...
什么是大模型框架？常用的大模型框架盘点对比
lvaolan的博客
1304
大模型框架是指用于训练、推理和部署大型语言模型（LLMs）的软件工具和库。这些框架通常提供了高效的计算资源管理、分布式训练、模型优化和推理加速等功能，以便更好地利用硬件资源（如GPU和TPU）来处理庞大的数据集和复杂的模型结构。前排提示，文末有大模型AGI-CSDN独家资料包哦！大模型框架的优点高效性：通过优化计算和内存管理，这些框架能够显著提高训练和推理的速度。可扩展性：支持分布式训练，可以在多个GPU或TPU上运行，适用于大规模数据集和复杂任务。
透彻理解大模型框架：Transformer模型原理详解与机器翻译
zql1009的博客
1万+
已知神经网络权重：W_q, W_k, W_v。
基础篇| 大模型部署框架
12-19
这时候有大模型部署框架用武之地,大模型部署框架作为一种高效、灵活的部署方式,能够大大提高模型训练和部署的效率,降低模型在部署过程中的时间和成本。 02 部署框架对比 03 总结 从支持模型数量,以及各种特性来看, xinference框架特性最全,支持模型最多, 从易用性来说,ollama绝对适用于一些初学者。
大模型技术学习之——大模型常用架构以及技术难点_大模型技术架构-CSDN...
12-18
优点:统一框架便于跨任务的知识迁移,模型更具有通用性‍ 缺点:对生成任务过于依赖,可能不适合一些特定的理解任务‍‍ 基于Transformer架构的文本处理模型开发的人工智能机器人:‍‍‍‍‍‍‍ DistilBERT 简介 DistilBERT是BERT的精简版,通过蒸馏技术减小模型规模,同时保留了大部分性能‍‍‍‍ ...
OLLAMA：如何像云端一样运行本地大语言模型
songgz的专栏
9154
您是否曾发现自己被云端语言模型的网络所缠绕，渴望获得更本地化、更具成本效益的解决方案？那么，您的探索到此结束。欢迎来到 OLLAMA 的世界，这个平台将彻底改变我们与大型语言模型 (LLM) 的交互方式，让我们可以在本地运行这些模型。我们将深入探讨 OLLAMA 的复杂性，探索其功能、设置过程以及它如何改变你的项目。无论您是 Python 开发人员、网络开发爱好者，还是喜欢摆弄语言模型的人，本文都是您的一站式资源。OLLAMA 是一个尖端平台，旨在本地运行开源大型语言模型。
LLM并发加速部署方案（llama.cpp、vllm、lightLLM、fastLLM）
ZhengrongYue的博客
7012
LLLM并发加速部署方案（llama.cpp、vllm、lightLLM、fastLLM）
推荐收藏!九大最热门的开源大模型 Agent 框架来了_agent框架
12-20
用通俗易懂的方式讲解:最火的大模型训练框架 DeepSpeed 详解来了 用通俗易懂的方式讲解:这应该是最全的大模型训练与微调关键技术梳理 用通俗易懂的方式讲解:Stable Diffusion 微调及推理优化实践指南 用通俗易懂的方式讲解:大模型训练过程概述 用通俗易懂的方式讲解:专补大模型短板的RAG ...
大模型与LangChain框架:规模、能力与应用开发
12-18
LangChain,作为一个先进的大语言模型开发框架,能够无缝整合LLM模型(如对话模型、embedding模型等)、向量数据库、交互层Prompt、外部知识库以及外部代理工具,从而为用户提供构建自由且高效的LLM应用的能力。其核心组件包括: 1. 模型输入/输出(Model I/O):作为与语言模型交互的关键接口,确保数据能够准确、高效地流入和流...
【大模型】什么是大模型框架？常用的大模型框架盘点对比
yuuuuuuuk的博客
4985
大模型框架是指用于训练、推理和部署大型语言模型（LLMs）的软件工具和库。这些框架通常提供了高效的计算资源管理、分布式训练、模型优化和推理加速等功能，以便更好地利用硬件资源（如GPU和TPU）来处理庞大的数据集和复杂的模型结构。大模型框架的优点高效性：通过优化计算和内存管理，这些框架能够显著提高训练和推理的速度。可扩展性：支持分布式训练，可以在多个GPU或TPU上运行，适用于大规模数据集和复杂任务。灵活性：提供丰富的API和工具，使得研究人员和工程师可以方便地进行定制化开发。
2024年掌握这些大模型应用开发框架，不再失业
elite1991的博客
3212
如果你是新手对于LLM相关知识不清楚，只是想着试试，你可以从langchain开始，他们的文档对新手比较友好，一些概念也比较简单，代码也便于理解，可以帮助你入门简单的LLM应用。如果你已经具备相关LLM知识，并且具备coding能力，想要开发商业应用，你可以使用LlamaIndex，因为它帮你封装好了很多解决RAG问题的组件，你可以直接使用，但是你需要自己调式，修改一些prompt以满足自己应用的要求。
一文搞懂大模型框架:LangChain
12-20
LangChain 是一种创新框架,正在彻底改变我们开发由语言模型驱动的应用程序的方式。通过引入先进的原理,LangChain 正在重新定义传统 API 所能实现的限制。此外,LangChain 应用程序具有智能代理的特性,使语言模型能够与环境进行互动和自适应。 LangChain 由多个模块组成。正如其名称所示,LangChain 的主要目的是将这些模块进...
【AIGC调研系列】进行大模型调用开发使用的框架有哪些
zachary的博客
1849
这些框架和工具展示了AIGC大模型在不同领域的应用潜力，从文本处理到视频编辑，再到代码生成和多模态内容创作，为开发者提供了广泛的选择来满足他们的具体需求。
大模型应用开发框架【LLM】
新缸中之脑
4184
随着人工智能的能力，特别是大型语言模型 (LLM) 的不断发展和演变，开发人员正在寻求将 AI 功能整合到他们的应用程序中。虽然文本完成和摘要等简单任务可以通过直接调用 OpenAI 或 Cohere 提供的 API 来处理，但构建复杂的功能需要付出努力和工具。推荐：用快速搭建3D场景。Jon Turow 和他在 Madrona 的团队首先指出了这一点，他们指出开发人员必须为提示工程、微调、蒸馏以及组装和管理将查询引用到适当端点的管道等步骤发明自己的工具。
推荐收藏！九大最热门的开源大模型 Agent 框架来了
机器学习社区
1万+
在人工智能领域，AI Agent 扮演着关键角色，能够模拟人类的智能行为。近年来，开源社区涌现出多个优秀的 AI Agent 框架，本文将介绍九种备受关注的开源AI Agent框架，包括AutoGPT、AutoGen、Langfuse、ChatDev、BabyAGI、CAMEL、SuperAGI、MetaGPT和ShortGPT。这些框架为开发者提供了丰富的资源和工具，为智能应用的开发和创新提供了强大支持。
LLMCompiler: 并行函数调用的大型语言模型优化框架及其应用
最新发布
12-04
内容概要：本文介绍了LLMCompiler，一种用于并行函数调用的大型语言模型（LLM）优化框架。LLMCompiler通过三个组件：LLM Planner、Task Fetching Unit 和 Executor，自动识别任务依赖关系并执行并行化任务。这显著...
一个基于 Python 的 LLM (大语言模型) 快速推理和服务框架，GPU利用率大幅提升
08-05
一个基于 Python 的 LLM (大语言模型) 推理和服务框架，以其轻量级设计、易于扩展和高速性能而著称。 利用了众多备受推崇的开源实现的优势，包括但不限于 FasterTransformer、TGI、 VLLM 和FlashAttention - 三进程...
大模型部署-使用OpenVINO本地化部署LLaMa3-附项目源码+流程教程-优质项目实战.zip
06-07
本项目将介绍如何使用OpenVINO进行大模型的本地化部署，特别是针对LLaMa3模型的实例。通过该项目，开发者可以学习到如何将复杂的模型部署到边缘设备，以便在本地运行高效且实时的推理任务。 LLaMa3是一种大规模的预...
LLM大模型推理加速实战：vllm、fastllm与llama.cpp使用指南
weixin_41888295的博客
3368
随着大语言模型（LLM）的兴起，推理加速成为关键。本文将介绍vllm、fastllm和llama.cpp三个加速工具的使用教程，并总结大模型推理的常见策略，为非专业读者提供简明易懂的操作建议。
大模型推理加速框架vllm部署的实战方案
热门推荐
herosunly的博客
3万+
本文主要介绍了大模型推理加速框架vllm部署的实战方案，希望对学习大语言模型的同学们有所帮助。 文章目录 1. 前言 2. 配置环境 2.1 安装虚拟环境 2.2 安装依赖库 3. 运行vllm
大模型应用开发框架
AI炼丹师的专栏
529
目前流行的大模型应用开发框架Llama-Index、LangChain等，都包含了RAG的部分。但是框架数量虽然多，在实际部署时却一言难尽，工程搭建繁琐，效果不理想，企业可能搞了半年都上不了线。
大模型应用的6种架构模式，你知道几种？
musicml的博客
3200
▼最近直播超级多，预约保你有收获架构设计模式已成为程序员的重要技能。然而，当我们转向大模型应用领域，情况可能会有所不同。面对新兴技术，比如：生成式 AI，我们尚缺乏成熟的设计模式来支撑这些解决方案。根据多年的架构设计经验，我在这里整理总结了一些针对大模型应用的设计方法和架构模式，试图应对和解决大模型应用实现中的一些挑战，比如：成本问题、延迟问题以及生成的幻觉等问题。—1—路由分发架构模式当用户输...
国内外 30 个热门大模型的架构的图文解析汇总
2301_76161259的博客
3663
在近两年内，有关 LLM 的研究进展很快，每天几乎都有新的语言模型发布（隐藏的 GPT-5，Llama3，Qwen1.5，Mixtral 8x22B 和 Claude 3 等等等等），它们的性能和效果似乎每天都在持续提升。然而，令人震惊的是，大多数现代 LLM 所使用的架构与最初的 GPT 模型非常相似。从模型架构角度出发，LLM 的一个关键组成部分一直保持不变，那就是 Transformer 架构的 Decoder。
 ollama大模型
07-31
对不起，您的问题中提到的"ollama大模型"似乎不是一个广为人知的概念。Ollama并非公开知名的大规模语言模型，可能是特定领域或小众项目中的术语。通常，像ChatGPT这样的大型语言模型是由像OpenAI这样的公司开发的，而它们的名称通常是更通用、易于识别的。 如果你是指类似的语言模型或技术平台，可以提供更多的上下文，我会更好地帮助解答。如果"ollama大模型"实际上是一个错误拼写或独特的术语，也请提供正确的信息。

    关于我们
    招贤纳士
    商务合作
    寻求报道
    400-660-0108
    kefu@csdn.net
    在线客服
    工作时间 8:30-22:00

    公安备案号11010502030143
    京ICP备19004658号
    京网文〔2020〕1039-165号
    经营性网站备案信息
    北京互联网违法和不良信息举报中心
    家长监护
    网络110报警服务
    中国互联网举报中心
    Chrome商店下载
    账号管理规范
    版权与免责声明
    版权申诉
    出版物许可证
    营业执照
    ©1999-2024北京创新乐知网络技术有限公司

算力资源比较多
码龄1年
北京北方算力智联科技有限责任公司

161
    原创

38万+
    周排名

1万+
    总排名

23万+
    访问

    等级

4147
    积分

1985
    粉丝

2496
    获赞

5
    评论

1776
    收藏

新秀勋章
持之以恒
1024勋章
勤写标兵
笔耕不辍
话题达人
创作能手
私信
关注
写文章
热门文章

    国内人工智能AI头部公司32家（包括详细技术、特点和综合实力） 14764
    英伟达A100、A800、H100、H800、V100以及RTX 4090的详细性能参数对比 13281
    介绍三种大模型：自然语言处理（NLP）大模型-计算机视觉（CV）大模型-多模态大模型 6368
    大模型框架汇总：大模型框架Ollama、大模型框架vLLM、大模型框架LightLLM、大模型框架llama.cpp、大模型框架LocalAI、大模型框架veGiantModel 6279
    英伟达（NVIDIA）H100性能及应用场景 5231

分类专栏

    人工智能
    49篇
    大模型
    109篇
    英伟达
    33篇
    算力
    111篇
    智能交通
    3篇
    昇腾910
    7篇
    医疗大模型
    3篇
    文心一言
    3篇
    CUDA
    5篇
    Transformer
    1篇
    算力集群
    1篇
    机器人
    2篇
    工业领域
    1篇
    智能电网
    1篇
    华为
    6篇
    动漫
    1篇
    教育类大模型
    1篇
    智谱华章
    2篇
    智算
    17篇
    B200
    2篇
    英特尔
    1篇
    H100
    1篇
    马斯克
    1篇
    Intel
    2篇
    差距
    超算
    5篇
    AMD
    1篇
    寒武纪
    1篇
    国产GPU
    6篇
    壁仞科技
    1篇
    天数智芯
    2篇
    沐曦
    1篇
    摩尔线程
    1篇
    渲染
    1篇
    云计算
    3篇

